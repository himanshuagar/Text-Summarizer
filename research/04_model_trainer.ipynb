{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3becd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cfcd974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/himanshuagarwal/Desktop/Text-Summarizer'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca5c3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/himanshuagarwal/Desktop/Text-Summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /users/himanshuagarwal/Desktop/Text-Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f75c7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/himanshuagarwal/Desktop/Text-Summarizer'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a37aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b421b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622cf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            weight_decay=float(params.weight_decay),\n",
    "            logging_steps=int(params.logging_steps),\n",
    "            evaluation_strategy=params.evaluation_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_steps=int(params.save_steps),  # CONVERT TO INT\n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps)\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6005766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6454775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        \n",
    "        # Truncation settings\n",
    "        max_input_length = 512\n",
    "        max_output_length = 128\n",
    "        \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        \n",
    "        def truncate_example(example):\n",
    "            return {\n",
    "                'input_ids': example['input_ids'][:max_input_length],\n",
    "                'attention_mask': example['attention_mask'][:max_input_length],\n",
    "                'labels': example['labels'][:max_output_length]\n",
    "            }\n",
    "        \n",
    "        train_dataset = dataset_samsum_pt[\"train\"].select(range(50)).map(truncate_example)\n",
    "        eval_dataset = dataset_samsum_pt[\"validation\"].select(range(20)).map(truncate_example)\n",
    "\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model_pegasus,\n",
    "            padding='longest'  # Efficient dynamic padding\n",
    "        )\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            eval_strategy=self.config.evaluation_strategy,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            gradient_checkpointing=False,  # Disabled for MPS\n",
    "            fp16=False  # Enable mixed precision\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1549d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-18 11:15:54,519: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-18 11:15:54,522: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-18 11:15:54,522: INFO: common: created directory at: artifacts]\n",
      "[2025-08-18 11:15:54,523: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/q9ssh4wj7r3d99nqq2qzhkb80000gn/T/ipykernel_83925/1228116770.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_trainer_config\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     model_trainer_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_trainer_config()\n\u001b[1;32m      4\u001b[0m     model_trainer_config \u001b[38;5;241m=\u001b[39m ModelTrainer(config\u001b[38;5;241m=\u001b[39mmodel_trainer_config)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel_trainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[29], line 48\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m seq2seq_data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\n\u001b[1;32m     27\u001b[0m     tokenizer,\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_pegasus,\n\u001b[1;32m     29\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Efficient dynamic padding\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m trainer_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     33\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mroot_dir,\n\u001b[1;32m     34\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_train_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Enable mixed precision\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_pegasus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq2seq_data_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     59\u001b[0m model_pegasus\u001b[38;5;241m.\u001b[39msave_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus-samsum-model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textS/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textS/lib/python3.10/site-packages/transformers/trainer.py:466\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textS/lib/python3.10/site-packages/transformers/trainer.py:5263\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires accelerate>1.3.0 to use Tensor Parallelism.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5264\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textS/lib/python3.10/site-packages/accelerate/accelerator.py:581\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins, parallelism_config)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdaa\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    580\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    582\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# FSDP2 doesn't use ShardedGradScaler, don't want to modify `get_grad_scaler`, rather create a simple utility\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea73a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Initializing configuration...\n",
      "[2025-08-18 11:26:04,320: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-18 11:26:04,322: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-18 11:26:04,323: INFO: common: created directory at: artifacts]\n",
      "[2025-08-18 11:26:04,323: INFO: common: created directory at: artifacts/model_trainer]\n",
      "🧠 Initializing model trainer...\n",
      "🚀 Starting training process...\n",
      "🚀 Using Apple Silicon GPU (MPS backend)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded dataset from artifacts/data_transformation/samsum_dataset\n",
      "📊 Training on 50 samples, validating on 20 samples\n",
      "⚙️ Training arguments configured:\n",
      "• Device: mps\n",
      "• Batch size: 1\n",
      "• Epochs: 1\n",
      "• Precision: full (fp32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.10/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7h/q9ssh4wj7r3d99nqq2qzhkb80000gn/T/ipykernel_83925/1939007638.py:137: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 01:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.10/site-packages/transformers/modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Training completed successfully!\n",
      "💾 Model saved to artifacts/model_trainer/pegasus-samsum-model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            weight_decay=float(params.weight_decay),\n",
    "            logging_steps=int(params.logging_steps),\n",
    "            evaluation_strategy=params.evaluation_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_steps=int(params.save_steps),\n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps)\n",
    "        )\n",
    "        return model_trainer_config\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        # Device detection with clear error handling\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "            print(\"🚀 Using Apple Silicon GPU (MPS backend)\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "            print(\"🚀 Using NVIDIA GPU (CUDA backend)\")\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            print(\"⚠️ Using CPU - training will be slow\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        \n",
    "        # Truncation settings to reduce memory usage\n",
    "        max_input_length = 512\n",
    "        max_output_length = 128\n",
    "        \n",
    "        # Load and process dataset\n",
    "        try:\n",
    "            dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "            print(f\"✅ Loaded dataset from {self.config.data_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load dataset: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        def truncate_example(example):\n",
    "            return {\n",
    "                'input_ids': example['input_ids'][:max_input_length],\n",
    "                'attention_mask': example['attention_mask'][:max_input_length],\n",
    "                'labels': example['labels'][:max_output_length]\n",
    "            }\n",
    "        \n",
    "        # Select smaller subsets for training and validation\n",
    "        train_dataset = dataset_samsum_pt[\"train\"].select(range(50)).map(truncate_example)\n",
    "        eval_dataset = dataset_samsum_pt[\"validation\"].select(range(20)).map(truncate_example)\n",
    "        print(f\"📊 Training on {len(train_dataset)} samples, validating on {len(eval_dataset)} samples\")\n",
    "\n",
    "        # Use dynamic padding to optimize memory usage\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model_pegasus,\n",
    "            padding='longest'\n",
    "        )\n",
    "\n",
    "        # Configure TrainingArguments - disable all mixed precision for MPS\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=1,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            eval_strategy=self.config.evaluation_strategy,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            gradient_checkpointing=False,  # Disabled for MPS compatibility\n",
    "            fp16=False,  # Disabled for MPS\n",
    "            bf16=False,  # Disabled for MPS\n",
    "            optim=\"adamw_torch\",  # Recommended optimizer\n",
    "            report_to=\"none\",     # Disable external logging\n",
    "            no_cuda=(device != \"cuda\")  # Explicitly disable CUDA if not using it\n",
    "        )\n",
    "\n",
    "        print(\"⚙️ Training arguments configured:\")\n",
    "        print(f\"• Device: {device}\")\n",
    "        print(f\"• Batch size: {self.config.per_device_train_batch_size}\")\n",
    "        print(f\"• Epochs: {self.config.num_train_epochs}\")\n",
    "        print(f\"• Precision: full (fp32)\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset\n",
    "        )\n",
    "        \n",
    "        print(\"🏁 Starting training...\")\n",
    "        try:\n",
    "            trainer.train()\n",
    "            print(\"🎉 Training completed successfully!\")\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"💥 Out of memory error! Try these solutions:\")\n",
    "                print(\"1. Reduce max_input_length (current: 512)\")\n",
    "                print(\"2. Reduce batch size (current: {self.config.per_device_train_batch_size})\")\n",
    "                print(\"3. Reduce dataset subset sizes (current: train 50, eval 20)\")\n",
    "                print(\"4. Use gradient checkpointing by setting gradient_checkpointing=True\")\n",
    "            raise\n",
    "        \n",
    "        # Save trained model and tokenizer\n",
    "        output_dir = os.path.join(self.config.root_dir, \"pegasus-samsum-model\")\n",
    "        try:\n",
    "            model_pegasus.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"💾 Model saved to {output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to save model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Training pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"🔍 Initializing configuration...\")\n",
    "        config_manager = ConfigurationManager()\n",
    "        model_trainer_config = config_manager.get_model_trainer_config()\n",
    "        \n",
    "        print(\"🧠 Initializing model trainer...\")\n",
    "        model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "        \n",
    "        print(\"🚀 Starting training process...\")\n",
    "        model_trainer.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⛔ Critical error: {str(e)}\")\n",
    "        print(\"💡 Troubleshooting tips:\")\n",
    "        print(\"- Check available memory with `free -h` or Activity Monitor\")\n",
    "        print(\"- Reduce model size or dataset size\")\n",
    "        print(\"- Update PyTorch and Transformers: `pip install --upgrade torch transformers`\")\n",
    "        print(\"- Try CPU fallback: set `device = 'cpu'`\")\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
